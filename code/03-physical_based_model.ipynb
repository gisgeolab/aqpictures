{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1687364-b0b8-4ff8-bc99-47586925bdca",
   "metadata": {},
   "source": [
    "# PM2.5 estimation from sky images (PPPC)\n",
    "This notebook script:\n",
    " - Extracts two “naturalness” features from sky images:\n",
    "   - **HS**: spatial entropy of saturation\n",
    "   - **ES**: wavelet-based (transform) entropy of saturation\n",
    " - Merges features with ground PM2.5 measurements\n",
    " - Fits reference distributions on “clean air” samples\n",
    " - Builds a combined quality score **Q**\n",
    " - Fits a logistic mapping to predict PM2.5 and evaluates performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc7be6b-e3d0-400f-9e5a-ee9fdf14cc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import imageio.v3 as iio\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pywt\n",
    "from scipy.stats import gumbel_r, norm\n",
    "from scipy.optimize import curve_fit\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c969c837-6205-476d-8e36-ba82187d77b4",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    " Functions for:\n",
    " - Parsing datetime from filename\n",
    " - Converting RGB → saturation\n",
    " - Computing spatial entropy (HS)\n",
    " - Computing wavelet-based transform entropy (ES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379cf141-5195-4618-b58d-d12a21fbdb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dt_from_name(name):\n",
    "    m = re.search(r'(\\d{8})-(\\d{4})', name)\n",
    "    if not m:\n",
    "        return None\n",
    "    ymd, hm = m.group(1), m.group(2)\n",
    "    return f\"{ymd[:4]}-{ymd[4:6]}-{ymd[6:8]} {hm[:2]}:{hm[2:4]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380c9218-2f36-4024-8ba8-028181274f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb_to_saturation(rgb_uint8):\n",
    "    rgb = rgb_uint8.astype(np.float32) / 255.0\n",
    "    R, G, B = rgb[..., 0], rgb[..., 1], rgb[..., 2]\n",
    "\n",
    "    U = np.maximum.reduce([R, G, B])\n",
    "    V = np.minimum.reduce([R, G, B])\n",
    "\n",
    "    S = np.zeros_like(U, dtype=np.float32)\n",
    "    mask = U > 0\n",
    "    S[mask] = (U[mask] - V[mask]) / U[mask]\n",
    "\n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660b0e93-c87c-440d-af51-89ee3cd840f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatial_entropy(S, bins=256):\n",
    "    hist, _ = np.histogram(S, bins=bins, range=(0, 1), density=True)\n",
    "    p = hist / (hist.sum() + 1e-12)\n",
    "    p = p[p > 0]\n",
    "    return float(-np.sum(p * np.log(p)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f9c0f1-6025-4308-a03c-7b7685e4a4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shannon_entropy_coeffs(coeffs, bins=256):\n",
    "    data = coeffs.ravel()\n",
    "    std = np.std(data)\n",
    "    rng = 6 * std if std > 0 else 1.0\n",
    "\n",
    "    hist, _ = np.histogram(data, bins=bins, range=(-rng, rng), density=True)\n",
    "    p = hist / (hist.sum() + 1e-12)\n",
    "    p = p[p > 0]\n",
    "\n",
    "    return float(-np.sum(p * np.log(p)))\n",
    "\n",
    "\n",
    "def transform_entropy(S, K=5, psi=4.0):\n",
    "    coeffs = pywt.wavedec2(S, wavelet='haar', level=K, mode='symmetric')\n",
    "    ES_scales = []\n",
    "\n",
    "    for k in range(1, K + 1):\n",
    "        cH, cV, cD = coeffs[k]\n",
    "\n",
    "        e_LH = shannon_entropy_coeffs(cH)\n",
    "        e_HL = shannon_entropy_coeffs(cV)\n",
    "        e_HH = shannon_entropy_coeffs(cD)\n",
    "\n",
    "        e_scale = (e_LH + e_HL + psi * e_HH) / (1 + psi)\n",
    "        ES_scales.append(e_scale)\n",
    "\n",
    "    return float(np.mean(ES_scales))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36caa433-92a1-49f4-a340-e9c8e1d4a67f",
   "metadata": {},
   "source": [
    "## Load images and extract features (HS, ES)\n",
    " - Reads all `.jpg`, `.png`, `.jpeg` images from `input/passed`\n",
    " - Crops a fixed sky region\n",
    " - Computes saturation, HS, and ES per image\n",
    " - Builds a dataframe indexed by Datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041fe363-7b5d-4787-bf2b-0739bcfe4d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder = \"Images\"\n",
    "image_paths = glob.glob(os.path.join(image_folder, \"*.jpg\")) + \\\n",
    "              glob.glob(os.path.join(image_folder, \"*.png\")) + \\\n",
    "              glob.glob(os.path.join(image_folder, \"*.jpeg\"))\n",
    "\n",
    "records = []\n",
    "\n",
    "for path in image_paths:\n",
    "    try:\n",
    "        fname = os.path.basename(path)\n",
    "        dt = parse_dt_from_name(fname)\n",
    "        if dt is None:\n",
    "            continue\n",
    "\n",
    "        img = iio.imread(path)\n",
    "        img = img[30:430, 30:1950, :3]  # sky crop\n",
    "\n",
    "        S = rgb_to_saturation(img)\n",
    "        HS = spatial_entropy(S)\n",
    "        ES = transform_entropy(S)\n",
    "\n",
    "        records.append({\"Datetime\": dt, \"HS\": HS, \"ES\": ES})\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {fname}: {e}\")\n",
    "\n",
    "df_img = pd.DataFrame(records)\n",
    "df_img[\"Datetime\"] = pd.to_datetime(df_img[\"Datetime\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13577a55-b451-4119-8e14-854d9afa855c",
   "metadata": {},
   "source": [
    "## Load PM2.5 measurements and merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea994147-4566-45c8-ac54-0dc7797dc1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pm = pd.read_csv(\"Model Input/PM25_MI Marche_ARPA.csv\")\n",
    "df_pm[\"Datetime\"] = pd.to_datetime(df_pm[\"Datetime\"], format=\"%d/%m/%Y %H:%M\")\n",
    "\n",
    "merged = df_img.merge(df_pm, on=\"Datetime\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8826a6a-1e44-47ab-a0b3-7705771c5412",
   "metadata": {},
   "source": [
    "## Clean-air subset for “naturalness” fitting\n",
    " Uses low-PM samples to estimate reference distributions (the “clean air” baseline)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1746acb0-da6e-4ab1-894a-f1cabcb84763",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = merged[merged[\"PM25\"] < 12]\n",
    "HS_clean = clean[\"HS\"].values\n",
    "ES_clean = clean[\"ES\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412b630a-3b8a-45ce-bb40-8585bf7a264c",
   "metadata": {},
   "source": [
    "## Fit reference distributions (baseline statistics)\n",
    " The code fits:\n",
    " - One feature with a **Gumbel** distribution (extreme value)\n",
    " - The other feature with a **Gaussian** distribution\n",
    "\n",
    " Important:\n",
    " The mapping (HS→Gumbel vs ES→Gumbel) depends on your assumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ef002d-14d8-4e32-a424-1db1db120f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spatial entropy → Extreme-value (Gumbel)\n",
    "#u, d = gumbel_r.fit(HS_clean)\n",
    "u, d = gumbel_r.fit(ES_clean)\n",
    "# Transform entropy → Gaussian\n",
    "#mu, sigma = norm.fit(ES_clean)\n",
    "mu, sigma = norm.fit(HS_clean)\n",
    "print(\"Gaussian parameters (ES): μ =\", mu, \", σ =\", sigma)\n",
    "print(\"Gumbel parameters (HS): u =\", u, \", d =\", d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23022439-967f-4179-82b9-6d05a0e3a7a7",
   "metadata": {},
   "source": [
    "## Visual sanity check: histogram + fitted PDFs\n",
    " Plots feature histograms for the clean subset and overlays the fitted probability density functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b4923a-6309-4286-965e-3984e97982d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create x-ranges for smooth PDF curves\n",
    "hs_range = np.linspace(HS_clean.min(), HS_clean.max(), 500)\n",
    "es_range = np.linspace(ES_clean.min(), ES_clean.max(), 500)\n",
    "\n",
    "# Evaluate fitted PDFs\n",
    "#hs_pdf = gumbel_r.pdf(hs_range, loc=u, scale=d)\n",
    "#es_pdf = norm.pdf(es_range, loc=mu, scale=sigma)\n",
    "\n",
    "hs_pdf = norm.pdf(hs_range, loc=u, scale=d)\n",
    "es_pdf = gumbel_r.pdf(es_range, loc=mu, scale=sigma)\n",
    "\n",
    "# Plot HS histogram + Gumbel fit\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.hist(HS_clean, bins=30, density=True, alpha=0.6, edgecolor=\"k\", label=\"HS histogram (clean)\")\n",
    "plt.plot(hs_range, hs_pdf, \"r-\", lw=2, label=\"Gumbel fit\")\n",
    "plt.xlabel(\"HS (Spatial Entropy)\")\n",
    "plt.ylabel(\"Probability Density\")\n",
    "plt.title(\"Naturalness Statistics of HS (Clean Air)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot ES histogram + Gaussian fit\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.hist(ES_clean, bins=30, density=True, alpha=0.6, edgecolor=\"k\", label=\"ES histogram (clean)\")\n",
    "plt.plot(es_range, es_pdf, \"b-\", lw=2, label=\"Gaussian fit\")\n",
    "plt.xlabel(\"ES (Transform Entropy)\")\n",
    "plt.ylabel(\"Probability Density\")\n",
    "plt.title(\"Naturalness Statistics of ES (Clean Air)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3c7c44-9468-415f-97a4-a356871ccb8c",
   "metadata": {},
   "source": [
    "## Build combined quality score Q\n",
    " Computes two likelihood-like terms:\n",
    " - QV from the Gaussian pdf\n",
    " - QG from the Gumbel pdf\n",
    "\n",
    " Then combines them as a weighted geometric mean:\n",
    " \\[\n",
    " Q = QV^w \\cdot QG^{(1-w)}\n",
    " \\]\n",
    " where `w=0.5` gives equal weighting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75141d9e-e445-403e-be78-92e2f3eabd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 0.5  # equal weighting\n",
    "\n",
    "merged[\"QV\"] = norm.pdf(merged[\"HS\"], loc=u, scale=d)\n",
    "merged[\"QG\"] = gumbel_r.pdf(merged[\"ES\"], loc=mu, scale=sigma)\n",
    "\n",
    "merged[\"Q\"] = (merged[\"QV\"] ** w) * (merged[\"QG\"] ** (1 - w))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753c323c-be89-41e1-b5e3-8d908636da2e",
   "metadata": {},
   "source": [
    "## Logistic calibration: Q → PM2.5\n",
    " Fits a logistic curve that maps the quality score **Q** to PM2.5.\n",
    " This is the regression step producing `PM25_pred`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee60e692-8897-46f4-9e36-b464a2dc3f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(Q, a1, a2, a3):\n",
    "    return a1 / (1 + np.exp(a2 - Q / a3))\n",
    "\n",
    "\n",
    "mask = np.isfinite(merged[\"Q\"]) & np.isfinite(merged[\"PM25\"])\n",
    "Q_vals = merged.loc[mask, \"Q\"].values\n",
    "PM_vals = merged.loc[mask, \"PM25\"].values\n",
    "\n",
    "popt, _ = curve_fit(\n",
    "    logistic,\n",
    "    Q_vals,\n",
    "    PM_vals,\n",
    "    p0=[PM_vals.max(), 1.0, np.median(Q_vals)]\n",
    ")\n",
    "\n",
    "merged.loc[mask, \"PM25_pred\"] = logistic(Q_vals, *popt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bae5ec-576a-4e65-a2ea-2cc32f76bdbd",
   "metadata": {},
   "source": [
    "## Evaluation metrics\n",
    " Reports:\n",
    " - MAE\n",
    " - MSE\n",
    " - R²\n",
    "\n",
    "## Diagnostic plot: measured vs predicted\n",
    " A 1:1 line is shown for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5509d584-d553-4a1d-9b8c-3229965c7c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = mean_absolute_error(PM_vals, merged.loc[mask, \"PM25_pred\"])\n",
    "mse = mean_squared_error(PM_vals, merged.loc[mask, \"PM25_pred\"])\n",
    "r2  = r2_score(PM_vals, merged.loc[mask, \"PM25_pred\"])\n",
    "\n",
    "print(\"MAE:\", mae)\n",
    "print(\"MSE:\", mse)\n",
    "print(\"R²:\", r2)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.scatter(PM_vals, merged.loc[mask, \"PM25_pred\"], alpha=0.6)\n",
    "plt.plot([0, PM_vals.max()], [0, PM_vals.max()], \"k--\")\n",
    "plt.xlabel(\"Measured PM2.5\")\n",
    "plt.ylabel(\"Predicted PM2.5\")\n",
    "plt.title(\"PPPC: Measured vs Predicted PM2.5\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413abfa4-4d63-4aab-aec9-a1f7c820d4d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
